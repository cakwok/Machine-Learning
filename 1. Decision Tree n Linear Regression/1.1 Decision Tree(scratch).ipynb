{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "1.1 Decision Tree(scratch).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1ZSVS6rczYdSYi11IxinLuXV2dSKys4-l",
      "authorship_tag": "ABX9TyPZ+mm+F/Fjkxp9GITC4/t4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "CS6140 Assignment 1 Q1.1\n",
        "Growing Decision Trees from scratch\n",
        "Wing Man, Kwok  \n",
        "05/18/2022\n",
        "'''"
      ],
      "metadata": {
        "id": "4gTKC9rLQLv0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "6H1blSxuZRLN"
      },
      "outputs": [],
      "source": [
        "#import libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split    #Library to split training and testing data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Build Classification Decision Tree, initialization of parameters, compute information gain, entropy, etc\n",
        "class ClassificationDecisionTree:                       \n",
        "    def __init__(self, max_depth=10, min_samples=2):    #initialize object paramenters.  Assume minimum samples in each axis rectangle is 2\n",
        "        self.max_depth = max_depth\n",
        "        self.min_samples = min_samples\n",
        "        self.root = None\n",
        "\n",
        "    def terminate(self, depth):                         #check if termination criteria is met\n",
        "        if (depth >= self.max_depth or self.numofSamples < self.min_samples or self.numofClasslabels == 1 ):\n",
        "            return True\n",
        "        return False\n",
        "           \n",
        "    def fit(self, X, y):                                #fit dataset into the tree    \n",
        "        self.root = self.build_tree(X, y)\n",
        "\n",
        "    def compute_entropy(self, y):                       #compute root/parent node entropy for information gain as splitting measure\n",
        "        entropy = 0\n",
        "        bin_propability = np.bincount(y) / len(y)       #count occurence of a value which matches with index, then normalize\n",
        "        for p in bin_propability:\n",
        "          if p > 0:\n",
        "            entropy += -p * np.log2(p)\n",
        "        return entropy\n",
        "\n",
        "    def split_datapoints_to_leftright(self, X, best_information_gain_X):\n",
        "        left_index = np.argwhere(X <= best_information_gain_X).flatten()     #split data points to left if smaller than variable X of best information gain\n",
        "        right_index = np.argwhere(X > best_information_gain_X).flatten()     #split data points to right if larger than variable X of best information gain\n",
        "        return left_index, right_index                                       #return location(index) of the data point split into left and right correspondingly\n",
        "\n",
        "    def compute_information_gain(self, X, y, best_information_gain_X):        #compute information gain of a branch\n",
        "        parent_loss = self.compute_entropy(y)\n",
        "        left_index, right_index = self.split_datapoints_to_leftright(X, best_information_gain_X)\n",
        "        \n",
        "        if len(left_index) == 0 or len(right_index) == 0: \n",
        "            return 0\n",
        "        \n",
        "        child_loss = (len(left_index) / len(y)) * self.compute_entropy(y[left_index]) + (len(right_index) / len(y)) * self.compute_entropy(y[right_index])\n",
        "        return parent_loss - child_loss\n",
        "\n",
        "    def find_max_information_gain(self, X, y, features):                      #find max information gain for each datapoint\n",
        "  \n",
        "        axis_aligned_rectangle_score = - 1\n",
        "        axis_aligned_rectangle_feature = None\n",
        "        axis_aligned_rectangle_iris_dimension = None\n",
        "\n",
        "        for col in features:                           #for each feature, source, eg, [3 1 2 0]\n",
        "  \n",
        "            X_col = X[:, col]                          #extract one column of X.  Format: X[row_index, column_index]\n",
        "            iris_dimensions = np.unique(X_col)         #returns the sorted unique elements of the X column.  each represents a data point of X column\n",
        "   \n",
        "            for iris_dimension in iris_dimensions:      #for each unique datapoint of iris species measurement\n",
        "                information_gain = self.compute_information_gain(X_col, y, iris_dimension) #calculate information gain\n",
        "                if information_gain > axis_aligned_rectangle_score:                        #store parameters of highest information gain\n",
        "                    axis_aligned_rectangle_score = information_gain\n",
        "                    axis_aligned_rectangle_feature = col\n",
        "                    axis_aligned_rectangle_iris_dimension = iris_dimension\n",
        "\n",
        "        return axis_aligned_rectangle_feature, axis_aligned_rectangle_iris_dimension\n",
        "    \n",
        "    def build_tree(self, X, y, depth=0):\n",
        "\n",
        "        self.numofSamples = X.shape[0]                    #X.shape = 105, 4 (105 training dataset, 4 features)\n",
        "        self.numofFeatures = X.shape[1]   \n",
        "        self.numofClasslabels = len(np.unique(y))         #return number of unique labels of column y (label)\n",
        "\n",
        "        #exit criteria\n",
        "        if self.terminate(depth):                         \n",
        "            predicted_label = np.argmax(np.bincount(y))   #return the max of count of elements value same as array index\n",
        "            return Node(value=predicted_label)\n",
        "\n",
        "        #iterate each data point, find the data point with maxiumn information gain\n",
        "        random_features = np.random.choice(self.numofFeatures, self.numofFeatures, replace=False) #generate 4 numbers, range 0 to 4, eg [0 1 2 3], [3 2 0 1]\n",
        "        best_feature, best_information_gain = self.find_max_information_gain(X, y, random_features) #locate the datapoint with best information gain\n",
        "\n",
        "        # populate children \n",
        "        left_index, right_index = self.split_datapoints_to_leftright(X[:, best_feature], best_information_gain)\n",
        "        left_child = self.build_tree(X[left_index, :], y[left_index], depth + 1)\n",
        "        right_child = self.build_tree(X[right_index, :], y[right_index], depth + 1)\n",
        "        return Node(best_feature, best_information_gain, left_child, right_child)\n",
        "\n",
        "    def traverse_tree(self, x, node):                           #to compute accuracy of prediction, input testing dataset, pass each row of X, then follow index of node.feature and traverse til reaching leaf\n",
        "        if node.is_leaf():                                      #then the leaf value is returned, and compare with value of y of the row\n",
        "            return node.value\n",
        "    \n",
        "        if x[node.feature] <= node.threshold:                   #follow the logic of building tree, so now look up the x variables of the node feature and compare the value with best information gain\n",
        "            return self.traverse_tree(x, node.left)\n",
        "        \n",
        "        return self.traverse_tree(x, node.right)\n",
        "        \n",
        "    def print_tree(self, node, depth):                          #print tree by pre-order traversal (root left first)\n",
        "      if node.is_leaf():\n",
        "          #print (depth * \"\\t\", \"node\", node)                   #print nodeID to counter-check correctness\n",
        "          print (depth * \"\\t\" + \"|-----\", \"Class\", node.value)\n",
        "          return node.value\n",
        "      \n",
        "      #print (depth * \"\\t\", \"node\", node) \n",
        "      print (depth * \"\\t\" + \"|-----\", \"feature\", node.feature, \"<=\", node.threshold )\n",
        "      #print (depth * \"\\t\", \"node.left\", node.left)\n",
        "      #print (depth * \"\\t\", \"node.right\", node.right)\n",
        "      \n",
        "      print (depth * \"\\t\" + \"|-----\", \"left \")\n",
        "      self.print_tree(node.left, depth+1)\n",
        "\n",
        "      print (depth * \"\\t\" + \"|-----\", \"right \")\n",
        "      self.print_tree(node.right, depth+1)\n",
        "\n",
        "    def predict(self, X):                                               #predict if decision tree returns a leaf node, of which value is same as true y label.\n",
        "        predictions = []\n",
        "        for x in X:                                                     #x represent each row of X\n",
        "          predictions.append(self.traverse_tree(x, self.root))\n",
        "        return np.array(predictions)\n",
        "        "
      ],
      "metadata": {
        "id": "ikGaa9tS1hBk"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#create decision nodes and leaf nodes\n",
        "class Node:                                                            \n",
        "    def __init__(self, feature=None, threshold=None, left=None, right=None, *, value=None):\n",
        "        self.feature = feature                                          #store feature which is found resulting best information gain\n",
        "        self.threshold = threshold                                      #store the value of feature column, which results the best information gain to split dataset\n",
        "        self.left = left                                                #store left child\n",
        "        self.right = right                                              #store right child\n",
        "        self.value = value                                              #store predicted label as a leaf node\n",
        "    \n",
        "    def is_leaf(self):\n",
        "      if self.value is None:\n",
        "        return False\n",
        "      return True"
      ],
      "metadata": {
        "id": "j2L4hHnh1pLQ"
      },
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#return accuracy of decision tree prediction\n",
        "def compute_accuracy(y_truelabel, y_predicted):                                        \n",
        "    accuracy = np.sum(y_truelabel == y_predicted) / y_truelabel.size\n",
        "    return accuracy"
      ],
      "metadata": {
        "id": "UJyPsmFB1XAr"
      },
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#prepare dataset\n",
        "dataset = pd.read_csv(\"/content/drive/My Drive/Colab Notebooks/CS6140 Assignment1/data.csv\")    "
      ],
      "metadata": {
        "id": "mXHvM-fy39um"
      },
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feature_cols = ['feature1', 'feature2', 'feature3', 'feature4']\n",
        "X = dataset[feature_cols]                         #Assign all feature columns into variable X\n",
        "y = dataset['class']                              #Assign all target columns into variable y\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(    #split dataset into training dataset and testing dataset\n",
        "    X, y, test_size=0.2, random_state=42                 #random_state parameter gives a fixed random seed\n",
        ")\n",
        "\n",
        "#fit the tree\n",
        "model = ClassificationDecisionTree(max_depth=10)\n",
        "model.fit(X_train.values , y_train.values)"
      ],
      "metadata": {
        "id": "DDvvb2VT4jY7"
      },
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#after training, fit testing values to compute accuracy\n",
        "y_predicted = model.predict(X_test.values)             \n",
        "\n",
        "print (\"predicted value by decision tree\\t\", y_predicted)\n",
        "print(\"true value of y\\t\\t\\t\\t\", y_test.values, \"\\n\")\n",
        "\n",
        "accuracy = compute_accuracy(y_test, y_predicted)     \n",
        "print(\"Accuracy:\", accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U3Y1dF5kGxfO",
        "outputId": "ad2120ad-c7e1-46a0-efe9-aeb9f2c4571a"
      },
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "predicted value by decision tree\t [2 0 0 2 1 1 1 2 1 2 0 0 1 0 0 2 0 2 1 2 0 1 1 0 1 2 1 2 0 0]\n",
            "true value of y\t\t\t\t [2 0 0 2 1 1 1 2 2 2 0 0 1 0 0 2 0 2 1 2 0 1 1 0 1 2 1 2 0 0] \n",
            "\n",
            "Accuracy: 0.9666666666666667\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For the below printout of decision tree, i have referenced the structure of sklearn and draw an equivalence.\n",
        "\n",
        "\"Feature x\", as shown below at each decision node, represents the feature column which has been calculated for the best information gain, then a value from that feature to split dataset into left and right branches.  \n",
        "\n",
        "Each leaf shows a \"class\" value for the decision."
      ],
      "metadata": {
        "id": "U9mkHmxZzU_5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#print decision tree\n",
        "model.print_tree(model.root, depth=0)                    "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hFk05_asfEpb",
        "outputId": "468bfaaf-7728-4f95-ca4f-ef89a18f2ffc"
      },
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "|----- feature 3 <= 0.6\n",
            "|----- left \n",
            "\t|----- Class 0\n",
            "|----- right \n",
            "\t|----- feature 3 <= 1.7\n",
            "\t|----- left \n",
            "\t\t|----- feature 2 <= 5.0\n",
            "\t\t|----- left \n",
            "\t\t\t|----- feature 0 <= 4.9\n",
            "\t\t\t|----- left \n",
            "\t\t\t\t|----- feature 1 <= 2.4\n",
            "\t\t\t\t|----- left \n",
            "\t\t\t\t\t|----- Class 1\n",
            "\t\t\t\t|----- right \n",
            "\t\t\t\t\t|----- Class 2\n",
            "\t\t\t|----- right \n",
            "\t\t\t\t|----- Class 1\n",
            "\t\t|----- right \n",
            "\t\t\t|----- feature 0 <= 6.0\n",
            "\t\t\t|----- left \n",
            "\t\t\t\t|----- Class 1\n",
            "\t\t\t|----- right \n",
            "\t\t\t\t|----- Class 2\n",
            "\t|----- right \n",
            "\t\t|----- feature 2 <= 4.8\n",
            "\t\t|----- left \n",
            "\t\t\t|----- feature 0 <= 5.9\n",
            "\t\t\t|----- left \n",
            "\t\t\t\t|----- Class 1\n",
            "\t\t\t|----- right \n",
            "\t\t\t\t|----- Class 2\n",
            "\t\t|----- right \n",
            "\t\t\t|----- Class 2\n"
          ]
        }
      ]
    }
  ]
}
